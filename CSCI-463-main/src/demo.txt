The task of semantic edge detection (SED) is aimed at both detecting visually salient edges and recognizing their categories,or more concretely, locating fine edges utlizing lowlevel features and meanwhile identifying semantic categories with abstracted high-level features. An intuitive way for a deep CNN model to achieve both targets is to integrate highlevel semantic features with low-level category-agnostic edge features via a fusion model, which is conventionally designed following a fixed weight fusion strategy, independent of the input, as illustrated in the top row in Figure 1. In many existing deep SED models [Yu et al., 2017; Liu et al., 2018b; Yu et al., 2018], fixed weight fusion of multi-level features is implemented through 1 x 1 convolution, where the learned convolution kernel serves as the fusion weights. However, this fusion strategy cannot fully exploit multi-level information, especially the low-level features. This is because, first, it applies the same fusion weights to all the input images and ignores their variations in contents, illumination, etc. The distinct properties of a specific input need be treated adaptively for revealing the subtle edge details.  besides, for a same input image, different spatial locations on the corresponding feature map convey different information, but the fixed weight fusion manner applies the same weights to all these locations, regardless of their different semantic classes or object parts. This would unfavorably force the model to learn universal fusion weights for all the categories and locations. Consequently, a bias would be caused toward high-level features, and the power of multilevel response fusion is significantly weakened.

"""
File Name: default_fsdp_config.yaml
Purpose: Configuring fully sharded data parallelism strategy for training LLMs, remove fsdp_transformer_layer_cls_to_wrap line for training Llama-v2-7B model
Author: Alessandro Dos Santos
Documented: 05/10/2024
"""

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch: NO_PREFETCH
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: true # whether to offload parameters and gradients to CPU. Offloading state dict and state dict's tensor values is done in FSDPPlugin FullStateDictConfig and OptimStateDictConfig, respectively.
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: 3
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: LongT5Block
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16'
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
main_process_port: 39500
